{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd      \n",
    "import numpy as np       \n",
    "import seaborn as sns   \n",
    "import matplotlib.pyplot as plt \n",
    "pd.set_option('display.max_rows', None)  \n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "import warnings     \n",
    "warnings.filterwarnings('ignore')\n",
    "import tweepy\n",
    "from textblob import TextBlob\n",
    "from wordcloud import WordCloud\n",
    "import re\n",
    "from datetime import date\n",
    "import plotly.express as px\n",
    "import tweepy\n",
    "from tweepy import Stream\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy.streaming import StreamListener\n",
    "import json\n",
    "import csv\n",
    "import sys\n",
    "import time\n",
    "import typing\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import typing\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twitter Account Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "consumer_key = 'mJTR0SDWEUuoFEHlWJEl7kIdY'\n",
    "consumer_secret_key = 'Qmd1mwcNP2csyrNNRp3YvEvQg0JUn3KbeUNAgIBk7jw9f3kQke'\n",
    "access_token = '1299380381158977538-5SnP9wIfkWGQnq0xvd1JR7A8u8JooI'\n",
    "access_token_secret = 'Rtj381olRgtl0Awm41tkoKIGrzI9dbX9z48uevBSlq2tj'\n",
    "\n",
    "authenticate = tweepy.OAuthHandler(consumer_key, consumer_secret_key)\n",
    "authenticate.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(authenticate, wait_on_rate_limit=True)\n",
    "places = api.geo_search(query=\"Australia\", granularity=\"country\")\n",
    "place_id = places[0].id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def api_connect(log):\n",
    "    consumer_key = log['API_KEY']\n",
    "    consumer_secret_key = log['API_SECRET_KEY']\n",
    "    access_token = log['ACCESS_TOKEN']\n",
    "    access_token_secret = log['ACCESS_TOKEN_SECRET']\n",
    "    try:\n",
    "        authenticate = tweepy.OAuthHandler(consumer_key, consumer_secret_key)\n",
    "        authenticate.set_access_token(access_token, access_token_secret)\n",
    "        api = tweepy.API(authenticate, wait_on_rate_limit=True)\n",
    "        api.verify_credentials()\n",
    "        places = api.geo_search(query=log['COUNTRY'], granularity=\"country\")\n",
    "        place_id = places[0].id\n",
    "        conn = 'Authentication OK'\n",
    "    except:\n",
    "        api =None\n",
    "        place_id= None\n",
    "        conn = \"Error during authentication\"\n",
    "    return api, conn, place_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweet Extraction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toDataFrame(tweets,hashtag):\n",
    "\n",
    "    DataSet = pd.DataFrame()\n",
    "\n",
    "    DataSet['tweetText'] = [tweet.text for tweet in tweets]\n",
    "    DataSet['tweetCreated'] = [tweet.created_at for tweet in tweets]\n",
    "    DataSet['userLocation'] = [tweet.user.location for tweet in tweets]\n",
    "    DataSet['Coordinates'] = [np.average(tweet._json['place']['bounding_box']['coordinates'][0], axis=0) for tweet in tweets]\n",
    "    tweets_place= []\n",
    "    for tweet in tweets:\n",
    "        if tweet.place:\n",
    "            tweets_place.append(tweet.place.full_name)\n",
    "        else:\n",
    "            tweets_place.append('null')\n",
    "    DataSet['TweetPlace'] = [i for i in tweets_place]\n",
    "    return DataSet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hashtag List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_hashtags = ['#Morrison','#climatechange','#Labor','#BlamingLabor','#NewsPoll',\n",
    "                 '#ScottyMustGo','#ScottyDoesNothing','#SelfishAlbo','#scottyfromarketing'\n",
    "                 ,'#ScottyFromPhotoOps','#LNPfail','#EnoughIsEnough','#ScoMo','#Albo2022',\n",
    "                 '#liberal','#ScottyFromMarketing','#policyfail','#ScottyFromCoverUps' ,'#NotMyPM' ,'#March4Justice',\n",
    "                 '#AlboForPM','#AFPraids','#ShameonLiberals', '#SelfishAlbo','#auspoll', \n",
    "                 '#auspol', '#COVID19', '#AusVotes2019', '#ausvotes','#onyourside', '#LeadershipFail'\n",
    "                ]\n",
    "labor_hashtags = ['#ScottyFromCoverUps', '#ScottyDoesNothing' ,'#NotMyPM' ,'#Labor',\n",
    "                 '#March4Justice', '#EnoughIsEnough' ,'#scottyfromarketing', '#ScottyFromPhotoOps'\n",
    "                 ,'#AlboForPM', '#Albo2022','#AFPraids','#ShameonLiberals']\n",
    "liberal_hashtags = ['#NewsPoll', '#BlamingLabor', '#climatechange', '#Morrison','#BlamingLabor', '#SelfishAlbo',\n",
    "                    '#ScoMo']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twitter Function Calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_neutral(api,place_id,log):\n",
    "    # import hashtags\n",
    "    list_hashtags = log['NEUTRAL_HASHTAGS'].dropna()\n",
    "    list_hashtags= list(list_hashtags)\n",
    "    \n",
    "    df_neutral = pd.DataFrame()\n",
    "    if (not api):\n",
    "        print (\"Can't Authenticate\")\n",
    "        sys.exit(-1)\n",
    "    else:\n",
    "        for tag in list_hashtags:\n",
    "            cursor = tweepy.Cursor(api.search, q= tag+\" ; place:%s\" % place_id,lang='en')\n",
    "            results=[]\n",
    "            for item in cursor.items():\n",
    "                results.append(item)\n",
    "\n",
    "            DataSet_mini = toDataFrame(results,tag)\n",
    "            df_neutral= df_neutral.append(DataSet_mini)\n",
    "            return df_neutral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_liberal(api,place_id,log):\n",
    "    # import hashtags\n",
    "    list_hashtags = log['LIBERAL_HASHTAGS'].dropna()\n",
    "    liberal_hashtags = list(list_hashtags)\n",
    "    \n",
    "    df_liberal = pd.DataFrame()\n",
    "    if (not api):\n",
    "        print (\"Can't Authenticate\")\n",
    "        sys.exit(-1)\n",
    "    else:\n",
    "        for tag in liberal_hashtags:\n",
    "            cursor = tweepy.Cursor(api.search, q= tag+\" ; place:%s\" % place_id,lang='en')\n",
    "            results=[]\n",
    "            for item in cursor.items():\n",
    "                results.append(item)\n",
    "\n",
    "            DataSet_mini = toDataFrame(results,tag)\n",
    "            df_liberal= df_liberal.append(DataSet_mini)\n",
    "            print(len(df_liberal.index))\n",
    "            return df_liberal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_labor(api,place_id,log):\n",
    "    list_hashtags = log['LABOR_HASHTAGS'].dropna()\n",
    "    labor_hashtags= list(list_hashtags)\n",
    "    \n",
    "    df_labor = pd.DataFrame()\n",
    "    if (not api):\n",
    "        print (\"Can't Authenticate\")\n",
    "        sys.exit(-1)\n",
    "    else:\n",
    "        for tag in labor_hashtags:\n",
    "            cursor = tweepy.Cursor(api.search, q= tag+\" ; place:%s\" % place_id,lang='en')\n",
    "            results=[]\n",
    "            for item in cursor.items():\n",
    "                results.append(item)\n",
    "\n",
    "            DataSet_mini = toDataFrame(results,tag)\n",
    "            df_labor= df_labor.append(DataSet_mini)\n",
    "            return df_labor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_neutral = pd.read_csv(\"neutral_tweet_extracted.csv\")\n",
    "df_labor = pd.read_csv(\"labor_tweet_extracted.csv\")\n",
    "df_liberal = pd.read_csv(\"liberal_tweet_extracted.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Local Dataset, Cleaning and Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_result():\n",
    "    df_local = pd.read_csv('data6378341312468407048.csv')\n",
    "    df_local = df_local[['fraser_annings_conservative_national_party_ordinary_votes',' socialist_alliance_ordinary_votes',\n",
    "                  ' labor_ordinary_votes', ' the_greens_ordinary_votes', ' national_party_ordinary_votes',\n",
    "                 ' liberal_ordinary_votes', ' united_australia_party_ordinary_votes',\n",
    "                  ' katters_australian_party_kap_ordinary_votes',\n",
    "                ' latitude', ' longitude', ' division_id', ' division_name', ' polling_place_name', ' state']]\n",
    "    df_local.isna().sum()/df_local.shape[0]*100<69\n",
    "    df_local.drop([' socialist_alliance_ordinary_votes', ' national_party_ordinary_votes', \n",
    "               ' katters_australian_party_kap_ordinary_votes'],\n",
    "              axis=1, inplace=True)\n",
    "    vote_result_division_wise = df_local.groupby(' division_name')[['fraser_annings_conservative_national_party_ordinary_votes',\n",
    "              ' labor_ordinary_votes', ' the_greens_ordinary_votes',\n",
    "             ' liberal_ordinary_votes', ' united_australia_party_ordinary_votes']].sum()\n",
    "    lat_lon = df_local.groupby(' division_name')[[' latitude',' longitude']].agg(np.median)\n",
    "    result = lat_lon.join(vote_result_division_wise, how='outer')\n",
    "    result = result.loc[['Brisbane', 'Canberra', 'Adelaide', 'Melbourne', 'Newcastle', 'Perth', 'Sydney', 'North Sydney'], :]\n",
    "    result = result.reset_index()\n",
    "    result.columns = ['Division Name', 'Latitude', 'Longitude',\n",
    "       'Conservative National Party',\n",
    "       'Labor Party', 'Greens Party',\n",
    "       'Liberal Party', 'United Australia Party']\n",
    "    result.head(20)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_local.drop([' socialist_alliance_ordinary_votes', ' national_party_ordinary_votes', \n",
    "               ' katters_australian_party_kap_ordinary_votes'],\n",
    "              axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vote_result_division_wise = df_local.groupby(' division_name')[['fraser_annings_conservative_national_party_ordinary_votes',\n",
    "              ' labor_ordinary_votes', ' the_greens_ordinary_votes',\n",
    "             ' liberal_ordinary_votes', ' united_australia_party_ordinary_votes']].sum()\n",
    "lat_lon = df_local.groupby(' division_name')[[' latitude',' longitude']].agg(np.median)\n",
    "result = lat_lon.join(vote_result_division_wise, how='outer')\n",
    "result = result.loc[['Brisbane', 'Canberra', 'Adelaide', 'Melbourne', 'Newcastle', 'Perth', 'Sydney', 'North Sydney'], :]\n",
    "result = result.reset_index()\n",
    "result.columns = ['Division Name', 'Latitude', 'Longitude',\n",
    "       'Conservative National Party',\n",
    "       'Labor Party', 'Greens Party',\n",
    "       'Liberal Party', 'United Australia Party']\n",
    "result.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Cleaning Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_links(text):\n",
    "    # Remove any hyperlinks that may be in the text starting with http\n",
    "    import re\n",
    "    return re.sub(r\"http\\S+\", \"\", text)\n",
    "\n",
    "def style_text(text:str):\n",
    "    # Convert to lowercase\n",
    "    return text.lower()\n",
    "\n",
    "def remove_words(text_data:str,list_of_words_to_remove: typing.List):\n",
    "    # Remove all words as specified in a custom list of words\n",
    "    return [item for item in text_data if item not in list_of_words_to_remove]\n",
    "\n",
    "def collapse_list_to_string(string_list):\n",
    "    # This is to join back together the text data into a single string\n",
    "    return ' '.join(string_list)\n",
    "\n",
    "def remove_apostrophes(text):\n",
    "    # Remove any apostrophes as these are irrelavent in our word cloud\n",
    "    text = text.replace(\"'\", \"\")\n",
    "    text = text.replace('\"', \"\")\n",
    "    text = text.replace('`', \"\")\n",
    "    text = re.sub(r'@\\S+', \"\",text)\n",
    "    text = re.sub(r'#\\S+',\"\",text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Twitter Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df_neutral,df_labor,df_liberal):\n",
    "    nltk.download('stopwords')\n",
    "    stopcorpus = stopwords.words('english')\n",
    "\n",
    "    df_neutral['cleaned_text'] = df_neutral['tweetText'].astype(str).apply(remove_links)\n",
    "    df_labor['cleaned_text'] = df_labor['tweetText'].astype(str).apply(remove_links)\n",
    "    df_liberal['cleaned_text'] = df_liberal['tweetText'].astype(str).apply(remove_links)\n",
    "\n",
    "    df_neutral['cleaned_text'] = df_neutral['cleaned_text'].astype(str).apply(style_text)\n",
    "    df_labor['cleaned_text'] = df_labor['cleaned_text'].astype(str).apply(style_text)\n",
    "    df_liberal['cleaned_text'] = df_liberal['cleaned_text'].astype(str).apply(style_text)\n",
    "\n",
    "    df_neutral['cleaned_text'] = df_neutral['cleaned_text'].astype(str).apply(lambda x: remove_words(x.split(),stopcorpus))\n",
    "    df_labor['cleaned_text'] = df_labor['cleaned_text'].astype(str).apply(lambda x: remove_words(x.split(),stopcorpus))\n",
    "    df_liberal['cleaned_text'] = df_liberal['cleaned_text'].astype(str).apply(lambda x: remove_words(x.split(),stopcorpus))\n",
    "\n",
    "\n",
    "    df_neutral['cleaned_text'] = df_neutral['cleaned_text'].apply(collapse_list_to_string)\n",
    "    df_labor['cleaned_text'] = df_labor['cleaned_text'].apply(collapse_list_to_string)\n",
    "    df_liberal['cleaned_text'] = df_liberal['cleaned_text'].apply(collapse_list_to_string)\n",
    "\n",
    "    df_neutral['cleaned_text'] = df_neutral['cleaned_text'].apply(remove_apostrophes)\n",
    "    df_labor['cleaned_text'] = df_labor['cleaned_text'].apply(remove_apostrophes)\n",
    "    df_liberal['cleaned_text'] = df_liberal['cleaned_text'].apply(remove_apostrophes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(text:str,desired_type:str='pos'):\n",
    "    # Get sentiment from text\n",
    "    sentiment_score = SentimentIntensityAnalyzer().polarity_scores(text)\n",
    "    return sentiment_score[desired_type]\n",
    "\n",
    "# Get Sentiment scores\n",
    "def get_sentiment_scores(df,data_column):\n",
    "    df[ '{} Positive Sentiment Score'.format(data_column)] = df[data_column].astype(str).apply(lambda x: get_sentiment(x,'pos'))\n",
    "    df['{} Negative Sentiment Score'.format(data_column)] = df[data_column].astype(str).apply(lambda x: get_sentiment(x,'neg'))\n",
    "    df['{} Neutral Sentiment Score'.format(data_column)] = df[data_column].astype(str).apply(lambda x: get_sentiment(x,'neu'))\n",
    "    df['{} Compound Sentiment Score'.format(data_column)] = df[data_column].astype(str).apply(lambda x: get_sentiment(x,'compound'))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('wordnet')\n",
    "def lemmatize_text(text):\n",
    "    w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)]\n",
    "\n",
    "def apply_lemm(df_neutral,df_liberal,df_labor):\n",
    "    df_neutral['clean_lemmatized'] = df_neutral['cleaned_text'].astype(str).apply(lemmatize_text)\n",
    "    df_labor['clean_lemmatized'] = df_labor['cleaned_text'].astype(str).apply(lemmatize_text)\n",
    "    df_liberal['clean_lemmatized'] = df_liberal['cleaned_text'].astype(str).apply(lemmatize_text)\n",
    "\n",
    "    df_neutral['clean_lemmatized'] = df_neutral['clean_lemmatized'].apply(collapse_list_to_string)\n",
    "    df_liberal['clean_lemmatized'] = df_liberal['clean_lemmatized'].apply(collapse_list_to_string)\n",
    "    df_labor['clean_lemmatized'] = df_labor['clean_lemmatized'].apply(collapse_list_to_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "apply_lemm()\n",
    "df_neutral = get_sentiment_scores(df_neutral,'clean_lemmatized')\n",
    "df_liberal = get_sentiment_scores(df_liberal,'clean_lemmatized')\n",
    "df_labor = get_sentiment_scores(df_labor,'clean_lemmatized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temp(df_neutral,df_liberal,df_labor,result):\n",
    "    result['temp']=1\n",
    "    df_labor['temp']=1\n",
    "    df_liberal['temp']=1\n",
    "    df_neutral['temp']=1\n",
    "    df_labor = pd.merge(result,df_labor,on='temp')\n",
    "    df_liberal = pd.merge(result,df_liberal,on='temp')\n",
    "    df_neutral = pd.merge(result,df_neutral,on='temp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining on the basis of coordinates and filtering under 20 km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_info(df_neutral,df_liberal,df_labor):\n",
    "    dfs = [df_neutral,df_liberal,df_labor]\n",
    "    cols = ['longitude_x','latitude_x']\n",
    "    for i in range(len(dfs)):\n",
    "        try:\n",
    "            dfs[i]['longitude_x'] = dfs[i]['Coordinates'].apply(str).str.strip('[]').str.split('-', expand=True)[0]\n",
    "            dfs[i]['latitude_x'] = '-'+dfs[i]['Coordinates'].apply(str).str.strip('[]').str.split('-', expand=True)[1]\n",
    "            dfs[i][cols] = dfs[i][cols].apply(pd.to_numeric, errors='coerce')\n",
    "            dfs[i]['distance'] = haversine_np(dfs[i]['longitude_x'],dfs[i]['latitude_x'],\n",
    "                                          dfs[i]['Longitude'],dfs[i]['Latitude'])\n",
    "            dfs[i] = dfs[i][dfs[i].distance < 20]\n",
    "            dfs[i] = dfs[i].sort_values('distance').drop_duplicates('cleaned_text').reset_index(drop=True)\n",
    "        except Exception:\n",
    "            pass\n",
    "    # try\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine_np(lon1, lat1, lon2, lat2):\n",
    "    \"\"\"\n",
    "    Calculate the great circle distance between two points\n",
    "    on the earth (specified in decimal degrees)\n",
    "\n",
    "    All args must be of equal length.    \n",
    "\n",
    "    \"\"\"\n",
    "    lon1, lat1, lon2, lat2 = map(np.radians, [lon1, lat1, lon2, lat2])\n",
    "\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "\n",
    "    a = np.sin(dlat/2.0)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2.0)**2\n",
    "\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    km = 6367 * c\n",
    "    return km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finalise(df_neutral,df_liberal,df_labor):\n",
    "    cols = ['longitude_x','latitude_x']\n",
    "\n",
    "    df_neutral[cols] = df_neutral[cols].apply(pd.to_numeric, errors='coerce')\n",
    "    df_labor[cols] = df_labor[cols].apply(pd.to_numeric, errors='coerce')\n",
    "    df_liberal[cols] = df_liberal[cols].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    df_neutral['distance'] = haversine_np(df_neutral['longitude_x'],df_neutral['latitude_x'],\n",
    "                                          df_neutral['Longitude'],df_neutral['Latitude'])\n",
    "    df_labor['distance'] = haversine_np(df_labor['longitude_x'],df_labor['latitude_x'],\n",
    "                                          df_labor['Longitude'],df_labor['Latitude'])\n",
    "    df_liberal['distance'] = haversine_np(df_liberal['longitude_x'],df_liberal['latitude_x'],\n",
    "                                          df_liberal['Longitude'],df_liberal['Latitude'])\n",
    "    \n",
    "    df_neutral = df_neutral[df_neutral.distance < 20]\n",
    "    df_labor = df_labor[df_labor.distance < 20]\n",
    "    df_liberal = df_liberal[df_liberal.distance < 20]\n",
    "    \n",
    "    df_neutral = df_neutral.sort_values('distance').drop_duplicates('cleaned_text').reset_index(drop=True)\n",
    "    df_labor = df_labor.sort_values('distance').drop_duplicates('cleaned_text').reset_index(drop=True)\n",
    "    df_liberal = df_liberal.sort_values('distance').drop_duplicates('cleaned_text').reset_index(drop=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_neutral = df_neutral[df_neutral.distance < 20]\n",
    "df_labor = df_labor[df_labor.distance < 20]\n",
    "df_liberal = df_liberal[df_liberal.distance < 20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_neutral = df_neutral.sort_values('distance').drop_duplicates('cleaned_text').reset_index(drop=True)\n",
    "df_labor = df_labor.sort_values('distance').drop_duplicates('cleaned_text').reset_index(drop=True)\n",
    "df_liberal = df_liberal.sort_values('distance').drop_duplicates('cleaned_text').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_neutral.to_json(\"neutral.json\",orient='records')\n",
    "df_labor.to_json(\"labor.json\",orient='records')\n",
    "df_liberal.to_json(\"liberal.json\",orient='records')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
